# Machine Learning with R

(EN)

This project presents a collection of solved exercises on Machine Learning algorithms using the R language and tidyverse libraries. These practice problems were part of the 2023.2 Machine Learning coursework at the Federal University of ABC (UFABC).
The topics are divided into 10 sets of exercices, each containing an .Rmd file with the R code and a corresponding .pdf file with the output, including tables, charts, and other visualizations. Each set includes the following subtopics:

1. **Introduction to R:**  
    *   Explore key features of base R, tidyverse and ggplot2 libraries;
    *   Perform calculations with matrices.
2. **k-Nearest Neighbors:**
    * Understand the curse of dimensionality;
    * Implement kNN from scratch;
    * Compare results for different values of k.
3. **Cross-Validation:**
    * Use cross-validation to determine optimal parameters and compare ML algorithms.
4. **Feature Selection:**
    * Understand p-values and the Variance Inflation Factor (VIF);
    * Examine the implications of collinearity;
    * Compare Backward Stepwise Selection (BSS) and Forward Stepwise Selection (FSS).
5. **Parameter Tuning:**
    * Implement "recipes" for preprocessing data;
    * Use Grid Search for parameter tuning;
    * Optimize Lasso, Ridge, Elastic-Net and Polynomial Regressions;
    * Analyze coefficient shrinkage in regularized regression methods.
6. **Kernels and Discriminant Analysis:**
    * Implement Gaussian and Polynomial Kernels;
    * Compare Linear Discriminant Analysis (LDA), Quadratic Discriminant Analysis (QDA), and Naive Bayes.
7. **Classification:**
    * Understand the Confusion Matrix and performance metrics (sensitivity, specificity, precision, and recall);
    * Optimize Logistic Regression and SVM using ROC curves and the Area Under the Curve (AUC);
    * Implement the Perceptron algorithm.
8. **Decision Trees:**
    * Implement the CART algorithm;
    * Discuss ensemble methods (Bagging and Boosting);
    * Apply Cost-Complexity Pruning;
    * Compare RandomForests and XGBoost.
9. **Neural Networks:**
    * Train Deep Neural Networks with the keras library;
    * Implement a Multilayer Perceptron;
    * Solve an image classification problem using data from the CIFAR-10 dataset.
10. **Dimensionality Reduction:**
    * Perform cluster analysis with k-means clustering, hierarchical clustering, and DBSCAN;
    * Select the optimal number of clusters using the elbow method and the Gap Statistic;
    * Apply PCA for dimensionality reduction.


<br>
<br>

(PT)

Este projeto apresenta uma coleção de listas de exercícios resolvidos sobre Aprendizado de Máquina utilizando a linguagem R e as bibliotecas do tidyverse. As soluções apresentadas foram desenvolvidas durante a disciplina de Aprendizado de Máquina na Universidade Federal do ABC (UFABC), no segundo quadrimestre de 2023. <br>
Os tópicos estão divididos em 10 listas de exercícios. Para cada lista, há um arquivo .Rmd com o código em R e um arquivo .pdf correspondente com tabelas, gráficos,visualizações e os demais elementos da saída. Abaixo estão os subtópicos abordados em cada lista:

1. **Introdução ao R:**
    * Introdução aos recursos do R e das bibliotecas tidyverse e ggplot2;
    * Cálculos com matrizes.
2. **kNN (k-Nearest Neighbors):**
    * Definição da maldição da dimensionalidade;
    * Implementação pura do kNN;
    * Análise de resultados para diferentes valores de k.
3. **Validação Cruzada:**
    * Uso da validação cruzada para escolher os melhores parâmetros e comparar algoritmos de Aprendizado de Máquina.
4. **Seleção de Características:**
    * Análise dos p-valores e do Fator de Inflação da Variância (VIF);
    * Implicações da multicolinearidade;
    * Comparação entre os métodos Backward Stepwise Selection (BSS) e Forward Stepwise Selection (FSS).
5. **Seleção de Parâmetros:**
    * "Receitas" para pré-processamento de dados;
    * Uso do Grid Search para seleção de parâmetros;
    * Otimização das Regressões Lasso, Ridge, Elastic-Net e Polinomial;
    * Análise da retração de coeficientes em métodos de regressão regularizados.
6. **Kernels e Análise Discriminante:**
    * Implementação de Kernels Gaussianos e Polinomiais;
    * Comparação entre Análise Discriminante Linear (LDA), Análise Discriminante Quadrática (QDA) e Naive Bayes.
7. **Classificação:**
    * Apresentação da Matriz de Confusão e métricas de desempenho (sensibilidade, especificidade, precisão e revocação);
    * Otimização da Regressão Logística e do SVM utilizando curvas ROC e AUC;
    * Implementação do algoritmo Perceptron.
8. **Árvores de Decisão (Decision Trees):**
    * Implementação do algoritmo CART;
    * Análise de métodos ensemble (Bagging e Boosting);
    * Aplicação do Cost-Complexity Pruning para otimização de árvores de decisão;
    * Comparação entre RandomForests e XGBoost.
9. **Redes Neurais:**
    * Treinamento de Redes Neurais Profundas com a biblioteca keras;
    * Implementação de um Perceptron Multicamadas (Multilayer Perceptron);
    * Solução de um problema de classificação de imagens com dados do conjunto CIFAR-10.
10. **Redução de Dimensionalidade:**
    * Agrupamento com k-means, agrupamento hierárquico e DBSCAN;
    * Seleção do melhor número de grupos utilizando o "método do cotovelo" e a Estatística de Gap;
    * Aplicação de Principal Component Analysis (PCA) para redução de dimensionalidade.


<br> <br>

## Resources / Materiais:
- An Introduction to Statistical Learning (with Applications in R): https://www.statlearning.com/
- The Elements of Statistical Learning: https://hastie.su.domains/ElemStatLearn/
- Tidy Modeling with R: https://www.tmwr.org/index.html
